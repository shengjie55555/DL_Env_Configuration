{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch教程\n",
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tensor Basics\n",
    "### 1.1 创建Tensor\n",
    "* ```torch.empty(size)```：没有初始化的tensor\n",
    "* ```torch.rand(size)```：随机初始化，[0, 1]\n",
    "* ```torch.zeros(size)```：全0\n",
    "* ```torch.tensor(tuple or list)```：从python的dict或者tuple创建\n",
    "* ```torch.from_numpy(np.array)```：从numpy的array创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.int64\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# torch.empty(size): uninitiallized\n",
    "x = torch.empty(3, 5)\n",
    "x = torch.empty_like(x)\n",
    "\n",
    "# torch.rand(size): random numbers [0, 1]\n",
    "x = torch.rand(3, 5)\n",
    "x = torch.rand(x.size())\n",
    "x = torch.rand_like(x)\n",
    "\n",
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "x = torch.ones(3, 5)\n",
    "x = torch.ones_like(x)\n",
    "\n",
    "# construct from python data\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# numpy <-> cpu tensor\n",
    "# If the Tensor is on the CPU (not the GPU), both objects will share the same memory location, so changing one will also change the other\n",
    "a = np.ones((3, 5))\n",
    "b = torch.from_numpy(a)\n",
    "c = b.numpy()\n",
    "\n",
    "# cpu <-> gpu\n",
    "x_cpu = torch.rand(3, 5)\n",
    "x_gpu = x_cpu.to(torch.device(\"cuda\"))\n",
    "x_gpu = torch.rand(3, 5, device=torch.device(\"cuda\"))\n",
    "\n",
    "# check size\n",
    "# check data type\n",
    "# check requires_grad\n",
    "print(x.size())\n",
    "print(x.dtype)\n",
    "print(x.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基本运算\n",
    "* 加减除：```+, -, /, add, sub, div```\n",
    "* 乘法：\n",
    "  * ```*, mul```：点乘\n",
    "  * ```mm, matmul```：矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1000, 1.0000],\n",
      "        [1.0000, 1.1000]])\n",
      "tensor([[1.1000, 1.0000],\n",
      "        [1.0000, 1.1000]])\n",
      "tensor([[ 0.9000, -1.0000],\n",
      "        [-1.0000,  0.9000]])\n",
      "tensor([[ 0.9000, -1.0000],\n",
      "        [-1.0000,  0.9000]])\n",
      "tensor([[10.,  0.],\n",
      "        [ 0., 10.]])\n",
      "tensor([[10.,  0.],\n",
      "        [ 0., 10.]])\n",
      "tensor([[0.1000, 0.0000],\n",
      "        [0.0000, 0.1000]])\n",
      "tensor([[0.1000, 0.0000],\n",
      "        [0.0000, 0.1000]])\n",
      "tensor([[0.1000, 1.0000],\n",
      "        [1.0000, 0.1000]])\n",
      "tensor([[0.1000, 1.0000],\n",
      "        [1.0000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 0], [0, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0.1, 1], [1, 0.1]], dtype=torch.float32)\n",
    "\n",
    "# elementwise addition, substraction, division\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "\n",
    "# multiplication\n",
    "# elementwise\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "# matrix product\n",
    "print(torch.mm(x, y))\n",
    "print(torch.matmul(x, y))  # broadcast supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "* 误差还没有反向传播时，grad默认为None，第一次传播之后grad变成Tensor类的实例化对象，而且会一直累加，因此需要手动zero_\n",
    "* 不需要追踪某个tensor的梯度时：\n",
    "  * ```with torch.no_grad():```\n",
    "  * ```x.detach_()```\n",
    "  * ```x.requires_grad_(False)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100: 100%|██████████| 100/100 [00:00<00:00, 1083.70it/s, loss is: 0.412968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063]], requires_grad=True)\n",
      "tensor([[0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063]])\n",
      "<SigmoidBackward object at 0x7fe4cf4ee820>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(3, 3, requires_grad=True)\n",
    "input = torch.tensor([[0.1, 0.7], [0.2, 0.9], [0.3, 0.6]], dtype=torch.float32)\n",
    "label = torch.tensor([0, 1])\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    output = torch.sigmoid(torch.matmul(weights, input).sum(dim=0))\n",
    "    loss = nn.L1Loss(reduction='mean')(output, label)\n",
    "    \n",
    "    if weights.grad is not None:\n",
    "        weights.grad.zero_()\n",
    "    loss.backward()  # backward() accumulates the gradient for this tensor into .grad attribute\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "    \n",
    "    epoch_loop.set_description(\"Epoch: %d\" % (epoch + 1))\n",
    "    epoch_loop.set_postfix_str(\"loss is: %.6f\" % loss.item())\n",
    "\n",
    "\n",
    "print(weights)\n",
    "weights.requires_grad_(False)  # weights.detach_()\n",
    "print(weights)\n",
    "print(output.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Pipeline\n",
    "* Design model\n",
    "* Construct loss and optimizer\n",
    "* Training loop:\n",
    "  * forward pass: compute prediction and loss\n",
    "  * backward pass: compute gradients\n",
    "  * update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------numpy implementation-----------------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 1890.35it/s, loss = 0.06237932, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 1-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 1513.37it/s, loss = 0.06237914, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 2-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 1093.76it/s, loss = 0.06237914, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 3-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 1453.43it/s, loss = 0.06237914, w = 1.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# numpy implementation\n",
    "class NumpyImplementation(object):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        self.w = w\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w * x\n",
    "    \n",
    "    def loss(self, y_predicted, y):\n",
    "        return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "    def grad(self, x, y_predicted, y):\n",
    "        return np.dot(2 * x, (y_predicted - y)) / len(x)\n",
    "\n",
    "    def update_weights(self, grad):\n",
    "        self.w -= self.lr * grad\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        print(\"numpy implementation\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            g = self.grad(x, y_pred, y)\n",
    "            self.update_weights(g)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using autograd to obtain grad\n",
    "class TorchImplementation1(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation1, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 1\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            l.backward()\n",
    "            with torch.no_grad():\n",
    "                self.update_weights(self.w.grad)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using optimizer to update weights\n",
    "class TorchImplementation2(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation2, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.w], self.lr)  # 需要更新的权重必须是可迭代的\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 2\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "\n",
    "# pytorch implementation: using nn.Linear to build network and nn.MSELoss to compute loss\n",
    "class TorchImplementation3(object):\n",
    "    def __init__(self, lr, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.model = nn.Linear(1, 1, bias=False)\n",
    "        nn.init.constant_(self.model.weight, 0)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x).unsqueeze(-1)\n",
    "        y = torch.from_numpy(y).unsqueeze(-1)\n",
    "\n",
    "        print(\"pytorch implementation version 3\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self.model(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l.item():.8f}, w = {self.model.state_dict()[\"weight\"].item():.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}')\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epoch = 20\n",
    "\n",
    "ni = NumpyImplementation(0.0, learning_rate, num_epoch)\n",
    "ni.train(x, y)\n",
    "\n",
    "ti1 = TorchImplementation1(0.0, learning_rate, num_epoch)\n",
    "ti1.train(x, y)\n",
    "\n",
    "ti2 = TorchImplementation2(0.0, learning_rate, num_epoch)\n",
    "ti2.train(x, y)\n",
    "\n",
    "ti3 = TorchImplementation3(learning_rate, num_epoch)\n",
    "ti3.train(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2000: 100%|██████████| 2000/2000 [00:00<00:00, 8179.02it/s, loss: 75.3618]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAg5klEQVR4nO3df5BdZZ3n8fe3OyTaCQgkMTJJbnd0gmtgXZCeFM7olCPMEiK7EWqdgmokI2IPBi3dYWbF7XVllJ6hVkdqXE2gXX5E0iXF7ozASBCBcqS0JkKjLCYgEiEtyUbzS4JJK0m6v/vHObf7nHvP/dHd595zf3xeVV3d9znn3n6aH9/73Of5Pt/H3B0REWkvHVl3QERE6k/BX0SkDSn4i4i0IQV/EZE2pOAvItKGFPxFRNrQrIO/mS03s++a2bNmtsPMPhG2n25mj5jZC+H308J2M7Mvm9lOM3vGzN4x2z6IiMj0pDHyPwFc7+6rgPOB68xsFXAD8Ji7rwQeCx8DXAysDL/6gU0p9EFERKZh1sHf3fe6+4/Cn38DPAcsBdYBm8PbNgPvD39eB3zdA9uAU83sjNn2Q0REqjcnzRczsx7gXOCHwBJ33xte+iWwJPx5KfBy5Gm7w7a9lLFo0SLv6elJs7siIi3tqaeeOuDui5OupRb8zWwB8I/AJ939VTObvObubmbTriNhZv0EU0PkcjlGRkbS6q6ISMszs9FS11LJ9jGzkwgC/7C7/1PY/Kv8dE74fV/YvgdYHnn6srCtiLsPuXuvu/cuXpz45iUiIjOQRraPAbcDz7n7lyKXHgDWhz+vB+6PtF8VZv2cDxyOTA+JiEgdpDHt80fAB4GfmNnTYdt/BW4G7jWzDwOjwJ+F17YCa4GdwBjwoRT6ICIi0zDr4O/u3wesxOULEu534LrZ/l4REZk57fAVEWlDCv4iIm1IwV9EpA0p+IuINKLhYe5c9Nc8an8KPT0wPJzqy6e6w1dERGbv5S9/k9wn+oA+AHzUoL8/uNjXl8rv0MhfRKSBXH015D5x6eTjX/HG4IexMRgYSO33KPiLiDSAH/8YzODOO4PHG/kojvFG9k/d9ItfpPb7FPxFRGpheDiYq+/oKDtnPzEB558P7whPNnnDG+Do8n/DR7m1+OZcLrXuKfiLiKRteDiYox8dBffge39/0RvAgw9CZyf88IdTj195Bbr+7jPQ1RV/za4uGBxMrYsK/iIiaRsYCObooyJz9mNjcMopcMklwaV3vhPGx2Ht2vDevj4YGoLu7mAuqLs7eJzSYi+ABdUWGl9vb6+rpLOINIWOjmDEX8iMTV+dYMOGqaYf/QjOPbc23TCzp9y9N7GLtfmVIiJtLGFufh+LMZ8K/FdfHbw/1CrwV6LgLyKStsHB2Jz99XyRJZNHmgRJO7ffnkXHpij4i4ikLZyzf3Dxn2M4X+J6AG6+ORjtL19e4fl1oOAvIlKNwtTNDRtKpnK6g13ZxyX775xsO3wYPvWpene6NAV/EZFKklI3N21KTOX8m78J3g/yLrssuOWUU7LrfhLV9hERqSQpdbPAkTHj5CvjqZhjY/D619eyYzOnkb+ISCUVyiqs4SFO5sjk4y98IRjtN2rgh5SCv5ndYWb7zGx7pO1GM9tjZk+HX2sj1z5tZjvN7HkzuyiNPoiI1EyJsgov0YPhPMyaybaJCfirv6pXx2YurZH/XRD566fc4u7nhF9bAcxsFXA5cFb4nI1m1plSP0RE0leQuglgOG/mpcnHW+ddim8ZxkqdaN5gUgn+7v44cKjK29cB97j7a+7+ErATWJ1GP0REUjc8PDXn39nJXazHiO/e9e4eLr79P6VafqHWar3g+zEzuwoYAa53918DS4FtkXt2h20iIo0ln+UzNoYDHeMnYpd/8AP4wz8E2JVB52anlgu+m4C3AOcAe4G/n+4LmFm/mY2Y2cj+/fsrP0FEJE3hiP9sfkJH4Wjf84G/OdUs+Lv7r9x93N0ngK8xNbWzB4jub1sWtiW9xpC797p77+LFi2vVVRGRRIdHX8FwdnD2ZNvLLMOt+RMla/YXmNkZkYeXAvlMoAeAy81snpmtAFYCT9SqHyIiM2EGp/JKrM0xlrEn1UNVspLKnL+ZfQN4D7DIzHYDnwXeY2bnAE4wIfYXAO6+w8zuBZ4FTgDXuft4Gv0QEZmtbduC+vpRv2Me8zgWPEj5UJWspBL83f2KhOaSNevcfRBo/n96ItJSCtM0e3rgpZuGYeCMYKNXLhcE/ibK6ilF5R1EpO3ddBN85jPxtqmzWPpaItgXav5VCxGRQlUeng7BaD8a+G+8MfkQrlajkb+ItJZIbj4wVXETYiP4M8+EF16IP7Udgn6eRv4i0lwqjeorHJ5+7Fgw2o8G/u9/v70CP2jkLyLNpJpRfakKnL/4RWLdnXYL+nka+YtI86gwqgcSc/B38hbMJ2Jtv/51+wZ+UPAXkWZSZlQ/qaACp+GsZGfsdnc49dQa9K+JKPiLSPMotbM22h4enn73wk8WVd+cmGjv0X6Ugr+INI+EuvpJO27tyj6uOnjL5OO+vvBQ9SaptV8PWvAVkeaRX9QdGEjccXv22bBjR/wpGuknU/AXkebSV7zj1j3I/IwaGoKPfKSO/WoyCv4i0tSUvjkzmvMXkaa0e3dx4N/O2Xh3T9lyDhJQ8BeR7E2jFg8EQX/58nibY5zFjqmNX3oDKEvBX0Syld+1OzoazNeUCd4bNxaP9o/lfh+noLFw45cUMW+SybHe3l4fGRnJuhsikraeniDgF+ruhl27Jh+WnNvv6Eie5DcLEvvbmJk95e69Sdc08heRbFXYtbtiRXHgd4/E+2o2fkkRBX8RyVaZ4G0WG/yzZk3CIL/KjV8Sp+AvItlKCN6GY6O7Ym3u8NBDCc8PyznQ3R18ROjuDh634OlbaUol+JvZHWa2z8y2R9pON7NHzOyF8PtpYbuZ2ZfNbKeZPWNm70ijDyLSpCLB+1VOKarHc999VeTt9/UFHxEmJoLvCvwVpTXyvwtYU9B2A/CYu68EHgsfA1wMrAy/+oFNKfVBRJpVXx82uos3cDjW7A7r1mXUpxaXSvB398eBQwXN64DN4c+bgfdH2r/ugW3AqWZ2Rhr9EJHmc//9xQu6Bw9ql26t1bK8wxJ33xv+/EtgSfjzUuDlyH27w7a9FDCzfoJPB+S0ci/SclSaITt1WfD1YDPBtP+VuvuQu/e6e+/ixYtr0DMRycJ731shfVNqrpbB/1f56Zzw+76wfQ8Q3Zi9LGwTkVaSVLJheBgz+O5347cq6NdfLYP/A8D68Of1wP2R9qvCrJ/zgcOR6SERaQUJJRvsyj7syoJSzF3z8S2qwZOFtFI9vwH8K/BWM9ttZh8Gbgb+1MxeAC4MHwNsBV4EdgJfAzak0QcRaSCRg9aPM6coffNT3BzU41ENnsyksuDr7leUuHRBwr0OXJfG7xWRBhWWZigM+kBxEbZS5R2kprTDV0RSN7LkfUWB/8ecUxz4QTV4MqKTvEQkVUEWzz/H2hyDk04CmwvHjk1dUA2ezGjkL9JupnlwSrUuv7w4ffN47i24dQT1du68E+64QzV4GoRG/iLtJJ+FEy7GTh6cArMKwqU3a/28+IKCfUPQyF+knUSycCaVyrip4hOCmTZrNSsFf5F2UuHglElVHK1YGPRXrAiDfo2mlSRdCv4i7aTaU6/KfEIoNdp/8UWmdR6vZEvBX6SdVHvqVcInhN0sLTpgZcuWgime6UwrSaa04CvSTvKLrQMDQYDP5YLAX7gIm8vFDlVP3KyVNK9f7bSSZE4jf5F2U82pV+EnhM/z34oC/6FDZRZ0dZh601DwF2lHlRZl+/qwsaP8dz4fa/au+Zy2tcz8vQ5TbxoK/iLtJmlR9sorYdGiyZLLRQu6wZHqlefvdZh60zBvkoTc3t5eHxkZybobIs2vpyc2nx9VVSE2s2DKSBqemT3l7r1J1zTyF2lVpaZ2EgK/hWP7qMnRfiHN37cEZfuItKJyZRw6O2F8HIDDnMKpHI49dR33cR+XJr+u5u9bhoK/SCsqlW+/fv1k4K9qiiequzs5LVSakqZ9RFpRqbz68XFuYqAo8H+PP64c+EulhUpT0shfpBUVbNLKqzja7+wM1giOH59q01RPS6r5yN/MdpnZT8zsaTMbCdtON7NHzOyF8Ptpte6HSNObTsG0gnz7pAXdcTqKA//mzUHdfaVqtrx6jfz/xN0PRB7fADzm7jeb2Q3h40/VqS8izWe6dfjzbevXY+Mnii67dcS36XZ1BesB0bIPd9+toN/Cap7nb2a7gN5o8Dez54H3uPteMzsD+Bd3f2u511Gev7S1Urn5+bn4BIkHrGBTgX7r1qlAv3ZtMOqPLhJ3dWnU3+SyzvN34Dtm9pSZhUMVlrj73vDnXwJLkp5oZv1mNmJmI/v3769DV0UaVLmCaQXTQRN3DycH/vxxikNDsHFjvL7P1q2qxtlm6jHyX+rue8zsjcAjwMeBB9z91Mg9v3b3svP+GvlLWys18l+4EH7728nAXXX1zUIdHck3ajdvU8t05O/ue8Lv+4BvAquBX4XTPYTf99W6HyJNrVTBNICxMR5iTVHg/+xnp3Gcoqpxtp2aBn8zm29mJ+d/Bv49sB14AFgf3rYeuL+W/RBpeFVU2SwqmLZ+PRw8iOGs5aHY7Y5x48ppnJ6lapztx91r9gW8Gfi/4dcOYCBsXwg8BrwAPAqcXum1zjvvPBdpSVu2uHd15c89D766uoL2UvcvXOidHI89BdwPclp1r1Hqdbu73c2C79N5rjQkYMRLxFRV9RTJ2nQyeTZsgFtvxbx4Hj5xh26ZbCBpfVln+4hIOaUyeUZH41NBGzZgmzYWBf6S1TfLvba0PQV/kayVWlQ1ix24Yps2Ft1Sth5PudeWtqfgL5KF6ALvwYPF180mU3WmVWs/Sgu2UoaCv0i9FR6jeORI/HoY+HfRXRT038ArpYP+/PmqySNVU/AXqYVyqZtJtfajPBjXr2BXvBnjFcrshRwbi+/aVeCXMhT8RdKWdEB6f//UG0CZRdhL+Oei0f7jvHtqtJ9UtyFP8/syDarnL5K2UqdoDQwEo/GZ1toHOP304HvhOoHm92WaNPIXSVul1M3R0dgIfloLuocOwYEDsGWL5vdlVjTyF0lbiZE9MNXuDmbVb9aKvjYEgV7BXmZBI3+RtA0Owty5ZW8xvHizlnWUD/ya2pEUKfiLpK2vD04+OfHSa8wtPbdfrtRKZ6emdiRVCv4itXDoUFGT4byO12Jt3jmnus1amzcr8EuqFPxFkkznsPQkkbTL2+gvGu3/HTfgc+fB+Hjp19BirtSQgr9IoUp5+vl7yr05hPXxDedabotdcowbFny1fOBfuFCbtaSmFPxFCpXL04eq3hzsyj5s7GjsJY7SNTXFMzZWPvi/+ur0P22ITIPq+YsUqnSeban6+/Pnw5EjyYenV5rXT6Ja/DJLqucvMh2VzrMtsYnLjhYH/qqqb5aiWvxSQwr+IoUqnWebL7EQkZi+2d2T/PpmwaeESlSrR2oos+BvZmvM7Hkz22lmN2TVD5EiSYelRzNufve7yVsTSzOEh+gmvomYwbXXwm23wUknle6DNnRJjWUS/M2sE/gqcDGwCrjCzFZl0ReRxMydvr7k8sjDw3D0KCOcVxT0V/KzYIonv1Cb9CZy992wcWNw7c47p64tXBh8Kb1T6iSr2j6rgZ3u/iKAmd0DrAOezag/0q7ymTv57J585g4kB9+BgcrVN/NZQQMDwbx9LhcE/cLXU30eyVBW0z5LgZcjj3eHbSL1VSmtEyY/GSy0g9jortit2zmreEE3/wZSbp+ASMYaesHXzPrNbMTMRvbv3591d6QVlcqoybeHnwxsdBeHWBi7xTHOSvqw2tlZ+Q1FJGNZBf89wPLI42VhW4y7D7l7r7v3Ll68uG6dkzaSkLkDTGbaJG3Wqpi+WWrzllI3pYFkFfyfBFaa2QozmwtcDjyQUV+kXQ0Pw+HDydfWrp3ZZq0LLggWbJModVMaSCbB391PAB8DHgaeA+519x1Z9EXa2MAAnDhR1Gw4tmljrK3iaL+zEz76UXj00cr7BEQaQGYnebn7VmBrVr9fpHAa5jCncCrFnwS8az6MFTUH8iUfovIZPNFsn8FBZfZIQ2noBV+RmopMwxheFPgnN2sNDQUj+wqvEVNqn4BIg1Dwl/ZRuJlr7Vr+uuPvi/L2b+fqYIonuuFr82ZN5UhL0QHu0h4SNnMVzutDwYJu0oYvTeVIi1BJZ2kPkTLMSTt0jx+HOb/fk1yqWaWVpUmVK+mskb+0h3BxN7E0g8fvKfVckVaiOX9pDYXz+Rs2xB6bTxRX38TiZZcr1fEXaSEK/tL8ko5V3LQJRkdx96J6PBDO7Rcu2Co/X9qIgr80v6TibARTPB1Jo33CEsqFZZMr1fEXaSEK/tL8CubkH2Rt0RTP+/hWPJNnwYLkoK78fGkTCv7S+PLz+WYwZ07wPZ+DD0WbtS7hwdjTHeNb/If4a2oRV9qcgr80tuh8PkxVzIzWyB8cTDxO8RcsL12PR4u40uaU6imNrcR8PjBZI7/kgm4pWsQV0chfGlyZ6RmjOJMnsfrmwoVaxBUpoJG/NKbh4WDUX2IHeuJmLesgoRkOHYIDB1LuoEhz08hfGk/hPH9E0ty+bxkO3iO0SUukahr5S+NJmOffy5v4PfYW3eoY9IcbswYH48XbQPP7IiUo+EvjKZjnT5ziic7r5w9HzxdfU+VNkYo07SONJ5ymuYavFQX+x7ggOZMn/4ahTVoiValZ8DezG81sj5k9HX6tjVz7tJntNLPnzeyiWvVBmlSYt38718Safcsw7+3+efJzNK8vMi21nva5xd2/GG0ws1XA5cBZwO8Bj5rZme4+XuO+SBMwA4iP1idyPdjfRqZvNK8vMmtZTPusA+5x99fc/SVgJ7A6g35IWgrLKefLLkzzPkuYzXHriLer+JpIKmo98v+YmV0FjADXu/uvgaXAtsg9u8M2aUYJxyMWHX1Y4T67sjhwT87re8Jr5r9EZMZmdYyjmT0KvCnh0gBBgD9A8L/v54Ez3P1qM/sKsM3dt4SvcTvwkLv/n4TX7wf6AXK53HmjSUfsSbYixyPGFB59mHDfOB3MoXi2r2RpBh2nKDItNTvG0d0vrLIDXwO+FT7cAyyPXF4WtiW9/hAwBMEZvjPvqdRMtUcfTjd9czq/S0SmrZbZPmdEHl4KbA9/fgC43MzmmdkKYCXwRK36ITVW7a7a8PG3eF9R4P/CF6oI/OV+l4hMWy3n/P+HmZ1DMO2zC/gLAHffYWb3As8CJ4DrlOnTxKrdVTs4WHpu/yvdMH8+HD1a+vcoo0ckVbOa86+n3t5eHxkZybobkiRfhK3Erto3vhH2748/5TeczAKOTDXMnQsnTgSbswp1d2unrsgM1GzOXwQom32TmL7Z3QOjR+KNx44FpZcXLFBpBpE6UHkHias2Z7/Cc82KA797WKG51MLtoUMqzSBSJxr5y5Rqc/YrPDcxkyfalMslp4dqQVekbjTylylJRybmK2ZW8VwbO1pca7+7p/g8lsHBYAE3qqsL1q6d+acOEZkWBX+ZUm3OfsLlwuMUP8C9QSZP0nOTSjSsXw+bNwefCNzjB7SLSOqU7SNTqt2tG5G4oBvN2a92V+4MfreIlFcu20cjf5lSajomIb/+s5c9UxT4f/a6t8cD/3Ry82f4qUNEZkbBX6ZEp2MAOjun5vwj0y9m8Llvvj32VO+az8oPvWvm1TZ1/q5IXSn4S1xf39QngPFw4/XoKHzwg8npm+GR6oyNwdatM0/VnManDhGZPQV/KZaQ9WNevPO2qB7PbKZoVKdfpK6U5y/FIkE8MWe/u6c2efqq0y9SNxr5S7FcjmOcVBT4/y3PBKN9TdGIND0Ffylio7uYx7FYm2M8w78LFoH7+oK8/M7O4GJnZ/BYo3aRpqHgL5N+8pPiBd0fsjo+tz8+HmT+bN48tSA8Pg6bNsGiRdqUJdIkFPxb2TSKtJnB2+PZmzjGap6MN3Z3J5eBADh4ULtyRZqEgn+ryhdaq1Au4YtfLB7tn6Cz9MlaR44kL/bmVVsLSEQypfIOraqKcgkVSzOUYkZxtbaC60mHsohIXam8QzsqUy5hyZIym7Wq4Z78zpGnXbkiDW9Wwd/MPmBmO8xswsx6C6592sx2mtnzZnZRpH1N2LbTzG6Yze+XMkoEYPMJ9u2benztnK9VH/Sj3IOTtwop5VOkKcx25L8duAx4PNpoZquAy4GzgDXARjPrNLNO4KvAxcAq4IrwXklbQS6+hWP7KO/uYdOJ/pm9fnc3HDgAW7ZoV65IE5rVDl93fw7AiqcA1gH3uPtrwEtmthNYHV7b6e4vhs+7J7z32dn0QxKEAfjADV9k8e4fxy49/ji8+91ARxXlGObPD0b50eye6Oheu3JFmlKt5vyXAi9HHu8O20q1y2yUSOm0K/uKAr97GPih8tx8Vxfcdptq7oi0oIrB38weNbPtCV/rat05M+s3sxEzG9m/f3+tf11zSkjp/JcP3120HnvkSEKCTlKZhvwTo0G+r08Hq4u0mIrTPu5+4Qxedw+wPPJ4WdhGmfak3z0EDEGQ6jmDfrS+gg1XhsNr8VtKZmXmg/jAQJAdlMsFbwgK7iItr1bTPg8Al5vZPDNbAawEngCeBFaa2Qozm0uwKPxAjfrQHsKUzk9yS/GCrpdPxwc0qhdpU7NN9bzUzHYD7wQeNLOHAdx9B3AvwULut4Hr3H3c3U8AHwMeBp4D7g3vlZnK5TCcf+CTk00f4o6g7LKISAmzCv7u/k13X+bu89x9ibtfFLk26O5vcfe3uvtDkfat7n5meE0J4dUosaB79tlBBc4ox7ij6+PKtReRsrTDt9ElLOge/8gGzGBH5DPT95b8GW4d8YXaaRR2E5H2opO8Gl3Sgu5v47cE8/r3xhvzbxr55+YLu4Hm9UVEI/+GFy7o/j/OKFrQPXjr/y69oJtUdlkVN0UkpJF/o8vliub1e3iJl3gz/GUXLDiWPJIvU9hNREQj/wb25JPFC7oTWBD4ofxIvtTuXVXcFBEU/BuWGaxePfX4NvpxrLj+ZqmRvA5ZF5EyFPwbzG23JdTad+jv/k7yE0qN5Pv6VJNHRErSnH+DcA8yMqOeeAL+4A/CB4OD8ewdqDySV8VNESlBI/8GcMUVxYHfPRL4QSN5EUmVRv4ZOnoUFiyIt+3fD4sWlXiCRvIikhKN/DPyutfFA//55wej/ZKBX0QkRRr519kLL8CZZ8bbTpyAzs5s+iMi7Ukj/zoyiwf+G28MRvsK/CJSbxr518GTT8Zz9qGKOvsiIjWkkX+NFW7W+va3FfhFJHsK/jWyZUt8s9bKlUHQv+ii0s8REakXTfukbHwc5hT8U923DxYvzqY/IiJJNPJP0fXXxwP/NdcEo30FfhFpNLMa+ZvZB4AbgbcBq919JGzvITij9/nw1m3ufm147TzgLuD1wFbgE+7NPQv+yitw2mnxtmPH4KSTMumOiEhFsx35bwcuAx5PuPZzdz8n/Lo20r4J+AiwMvxaM8s+ZGr16njgv/XWYLQfC/w6TlFEGsysRv7u/hyAFZahLMHMzgBOcfdt4eOvA+8HHir3vEb005/C294Wb0v8/KLjFEWkAdVyzn+Fmf3YzL5nZu8O25YCuyP37A7bmopZPPB/73tl0jd1nKKINKCKI38zexR4U8KlAXe/v8TT9gI5dz8YzvHfZ2ZnTbdzZtYP9APkGuAEqgcfhEsumXo8bx787ncVnqTjFEWkAVUM/u5+4XRf1N1fA14Lf37KzH4OnAnsAZZFbl0WtpV6nSFgCKC3tzezReGkWvu7dgVVlSvK5YKpnqR2EZGM1GTax8wWm1ln+PObCRZ2X3T3vcCrZna+BQsFVwGlPj00hJtvjgf+970veDOoKvCDjlMUkYY021TPS4H/CSwGHjSzp939IuCPgc+Z2XFgArjW3Q+FT9vAVKrnQzToYu/YGMyfH287cqS4raL8ou7AQDDVk8sFgV+LvSKSIWuWFPve3l4fGRmpy++69FK4776pxzfdpPVZEWk+ZvaUu/cmXVN5h4iXXy6eip+YKD5QXUSk2am8Q+iUU+KB//77g7l9BX4RaUVtP/L/wQ/gXe+KtzXJTJiIyIy1/si/TGkFs3jg37FDgV9E2kNrB/98aYXR0SCqh6UV/tc122LTOeeeG1xetSq7roqI1FNrT/sUlFY4zhzmjh2F26duOXgQTj89g76JiGSotUf+kRIK17KJuRyffPzxjwejfQV+EWlHrT3yz+UYG93HfOKF1Y7n3sKcL/88o06JiGSvtUf+g4OxwH8X6/Gu+cz5289l2CkRkey19sgfGJr/n3n2aI5b+EtYuBD+YUilFUSk7bVu8A8zfT4SraX/299m1x8RkQbSutM+OkRFRKSk1g3+OkRFRKSk1g3+pQ5L0SEqIiItHPx1iIqISEmtG/z7+mBoKDhyyyz4PqRMHxERaOVsHwgCvYK9iEiR1h35i4hISQr+IiJtSMFfRKQNKfiLiLQhBX8RkTZk3iTnFprZfmA0636UsAg4kHUnMtCufzfob2/Hv70Z/+5ud1+cdKFpgn8jM7MRd+/Nuh/11q5/N+hvb8e/vdX+bk37iIi0IQV/EZE2pOCfjqGsO5CRdv27QX97O2qpv1tz/iIibUgjfxGRNqTgnwIz+4KZ/dTMnjGzb5rZqVn3qV7M7ANmtsPMJsysZTIhSjGzNWb2vJntNLMbsu5PPZnZHWa2z8y2Z92XejKz5Wb2XTN7Nvxv/RNZ9ykNCv7peAQ4293fDvwM+HTG/amn7cBlwONZd6TWzKwT+CpwMbAKuMLMVmXbq7q6C1iTdScycAK43t1XAecD17XCv3cF/xS4+3fc/UT4cBuwLMv+1JO7P+fuz2fdjzpZDex09xfd/RhwD7Au4z7Vjbs/DhzKuh/15u573f1H4c+/AZ4Dlmbbq9lT8E/f1cBDWXdCamIp8HLk8W5aIAhI9cysBzgX+GHGXZm11j7MJUVm9ijwpoRLA+5+f3jPAMFHxOF69q3WqvnbRVqdmS0A/hH4pLu/mnV/ZkvBv0rufmG562b258AlwAXeYvmzlf72NrIHWB55vCxskxZnZicRBP5hd/+nrPuTBk37pMDM1gD/BfiP7j6WdX+kZp4EVprZCjObC1wOPJBxn6TGzMyA24Hn3P1LWfcnLQr+6fgKcDLwiJk9bWa3Zt2hejGzS81sN/BO4EEzezjrPtVKuKj/MeBhgkW/e919R7a9qh8z+wbwr8BbzWy3mX046z7VyR8BHwTeG/7//bSZrc26U7OlHb4iIm1II38RkTak4C8i0oYU/EVE2pCCv4hIG1LwFxFpQwr+IiJtSMFfRKQNKfiLiLSh/w+NtuqfmCPr2QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=4)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Linear(x.shape[1], y.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 2000))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        epoch_loop.set_description(f\"epoch: {(epoch + 1):d}\")\n",
    "        epoch_loop.set_postfix_str(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "y_pred = model(x).detach().cpu().numpy()\n",
    "plt.plot(x_numpy, y_numpy, \"ro\")\n",
    "plt.plot(x_numpy, y_pred, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100: 100%|██████████| 100/100 [00:00<00:00, 2798.59it/s, loss = 0.2480, acc = 0.9298]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            y_pred_cls = y_pred.round()\n",
    "            acc = y_pred_cls.eq(y_test).sum() / (y_test.shape[0])\n",
    "        epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "        epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10.000: 100%|██████████| 10/10 [00:00<00:00, 13.75it/s, loss = 0.1099, acc = 0.9737]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LogisticDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        bc = datasets.load_breast_cancer()\n",
    "        x, y = bc.data, bc.target\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        x_train = sc.fit_transform(x_train)\n",
    "        x_test = sc.transform(x_test)\n",
    "\n",
    "        self.x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        self.x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        self.y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "        self.y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.x_train[index], self.y_train[index]\n",
    "        else:\n",
    "            return self.x_test[index], self.y_test[index]\n",
    "\n",
    "train_dataset = LogisticDataset(train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = LogisticDataset(train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion= nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 10), leave=True)\n",
    "for epoch in epoch_loop:\n",
    "    num_batches = len(train_dataloader)\n",
    "    epoch_per_batch = 1.0 / num_batches\n",
    "\n",
    "    for i, (data, target) in enumerate(train_dataloader):\n",
    "        epoch += epoch_per_batch\n",
    "        epoch_loop.set_description(f'epoch: {epoch:.3f}')\n",
    "\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}')\n",
    "    if round(epoch) % 10 == 0:\n",
    "        num_corrects = 0\n",
    "        num_samples = 0\n",
    "        for i, (data, target) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(data)\n",
    "                y_pred_cls = y_pred.round()\n",
    "                num_corrects += y_pred_cls.eq(target).sum()\n",
    "                num_samples += y_pred_cls.shape[0]\n",
    "            acc = num_corrects / num_samples\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Softmax & Crossentropy\n",
    "* PyTorch默认的```nn.CrossEntropyLoss```，等同于```nn.LogSoftmax + nn.NLLLoss```，因此不需要在网络的最后使用```torch.softmax```，并且label的shape是```（n_samples, )```，不是one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41703007\n",
      "tensor(0.4170)\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_entropy_loss(x, y):\n",
    "    return -np.dot(y, np.log(x))\n",
    "\n",
    "out = np.array([2.0, 1.0, 0.1], dtype=np.float32)\n",
    "label = np.array([1, 0, 0], dtype=np.float32)\n",
    "pred = softmax(out)\n",
    "print(cross_entropy_loss(pred, label))\n",
    "\n",
    "out = torch.tensor([[2.0, 1.0, 0.1], [1.0, 2.0, 0.1]])  # shape: n_sample * n_cls\n",
    "label = torch.tensor([0, 1])  # shape: n_sample\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(out, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tensorboard\n",
    "* global_step：可以用于显示同一个tag下不同step的图片，随着epoch的增加，显示不同的预测结果\n",
    "* add_image：tensor或者array，shape：c * h * w\n",
    "* ```tensorboard --logdir ./runs/```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from einops import rearrange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if not os.path.exists(\"./runs\"):\n",
    "    os.mkdir(\"./runs\")\n",
    "\n",
    "writer = SummaryWriter(\"./runs\")\n",
    "\n",
    "# matplotlib.pyplot.figure\n",
    "fig = plt.figure(1)\n",
    "plt.plot([0, 1, 2], [2, 3, 4], '-')\n",
    "writer.add_figure(\"figure\", fig)\n",
    "\n",
    "# image: c * h * w\n",
    "img = rearrange(cv.imread(\"./materials/panda.jpg\"), 'h w c -> c h w')\n",
    "img2 = rearrange(cv.imread(\"./materials/panda2.jpg\"), 'h w c -> c h w')\n",
    "writer.add_image(\"images\", img, global_step=0)\n",
    "writer.add_image(\"images\", img2, global_step=1)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.BatchNorm1d(10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "input = torch.rand(4, 10)\n",
    "writer.add_graph(model, input_to_model=input)\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    writer.add_scalar(\"loss\", 2 * epoch + 1, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distributed Traning\n",
    "* 采用```DistributedDataParallel```训练时，如果从头训练负载相对均衡，而从checkpoint开始训练就出现负载不均衡，可以采用```state = torch.load('xxx.pth', map=torch.device('cpu'))```解决\n",
    "* 保存模型时，注意加```if dist.get_rank() == 0```来保证只在一个线程里面保存，不然后面load的时候会出错\n",
    "* 只需要在一个线程里面执行的操作，比如打印误差，日志等，也可以用```if dist.get_rank() == 0```来实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. index_add & scatter_add\n",
    "* ```x.index_add_(dim, index, source)```：将source中的每一个向量，分别加在x的不同向量上，具体加在x的哪一个向量上看index，具体计算过程如下：\n",
    "  * ```x[index[i], ...] += source[i, ...]  # index[i]所在位置取决于dim```\n",
    "* ```self_tensor.scatter_add_(dim, index_tensor, other_tensor)```：将other_tensor中的数据，按照index_tensor中的索引位置，添加至self_tensor中\n",
    "  * ```self_tensor[index[i][j], j] += other_tensor[i, j]  # index[i][j]所在位置取决于dim```\n",
    "* index_add & scatter_add底层实现是不确定性算法，即便设置了随机种子和cudnn，仍然无法保证可重复性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "---------------index_add----------------\n",
      "tensor([[0.2440, 0.2920, 0.9111, 0.5047],\n",
      "        [0.2882, 0.7742, 0.0946, 0.5450],\n",
      "        [0.3842, 0.5591, 0.1936, 0.1996]])\n",
      "tensor([[0.2440, 0.2920, 0.9111, 0.5047],\n",
      "        [0.6723, 1.3333, 0.2882, 0.7447],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "--------------scatter_add---------------\n",
      "tensor([[0.2699, 0.6904, 0.8482]])\n",
      "tensor([[0.2699, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.6904, 0.8482, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "index = torch.tensor([0, 1, 1], dtype=torch.int64)\n",
    "x = torch.zeros(3, 4, dtype=torch.float32)\n",
    "print(x)\n",
    "\n",
    "print(\"index_add\".center(40, '-'))\n",
    "source = torch.rand_like(x)\n",
    "print(source)\n",
    "print(x.index_add(0, index, source))\n",
    "\n",
    "print(\"scatter_add\".center(40, '-'))\n",
    "source = torch.rand_like(index.unsqueeze(0), dtype=torch.float32)\n",
    "print(source)\n",
    "print(x.scatter_add(0, index.unsqueeze(0), source))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a94075a2e62db5dc98a7ce177b0aa497782b90b7701cb2e0b55d059aa447695"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
