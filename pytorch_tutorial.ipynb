{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch教程\n",
    "## 分布式训练\n",
    "* 采用```DistributedDataParallel```训练时，如果从头训练负载相对均衡，而从checkpoint开始训练就出现负载不均衡，可以采用```state = torch.load('xxx.pth', map=torch.device('cpu'))```解决\n",
    "* 保存模型时，注意加```if dist.get_rank() == 0```来保证只在一个线程里面保存，不然后面load的时候会出错\n",
    "* 只需要在一个线程里面执行的操作，比如打印误差，日志等，也可以用```if dist.get_rank() == 0```来实现"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a94075a2e62db5dc98a7ce177b0aa497782b90b7701cb2e0b55d059aa447695"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('motion')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
