{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch教程\n",
    "## 0. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Tensor Basics\n",
    "### 1.1 创建Tensor\n",
    "* ```torch.empty(size)```：没有初始化的tensor\n",
    "* ```torch.rand(size)```：随机初始化，[0, 1]\n",
    "* ```torch.zeros(size)```：全0\n",
    "* ```torch.tensor(tuple or list)```：从python的dict或者tuple创建\n",
    "* ```torch.from_numpy(np.array)```：从numpy的array创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.int64\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# torch.empty(size): uninitiallized\n",
    "x = torch.empty(3, 5)\n",
    "x = torch.empty_like(x)\n",
    "\n",
    "# torch.rand(size): random numbers [0, 1]\n",
    "x = torch.rand(3, 5)\n",
    "x = torch.rand(x.size())\n",
    "x = torch.rand_like(x)\n",
    "\n",
    "# torch.zeros(size), fill with 0\n",
    "# torch.ones(size), fill with 1\n",
    "x = torch.ones(3, 5)\n",
    "x = torch.ones_like(x)\n",
    "\n",
    "# construct from python data\n",
    "x = torch.tensor([1, 2, 3])\n",
    "\n",
    "# numpy <-> cpu tensor\n",
    "# If the Tensor is on the CPU (not the GPU), both objects will share the same memory location, so changing one will also change the other\n",
    "a = np.ones((3, 5))\n",
    "b = torch.from_numpy(a)\n",
    "c = b.numpy()\n",
    "\n",
    "# cpu <-> gpu\n",
    "x_cpu = torch.rand(3, 5)\n",
    "x_gpu = x_cpu.to(torch.device(\"cuda\"))\n",
    "x_gpu = torch.rand(3, 5, device=torch.device(\"cuda\"))\n",
    "\n",
    "# check size\n",
    "# check data type\n",
    "# check requires_grad\n",
    "print(x.size())\n",
    "print(x.dtype)\n",
    "print(x.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基本运算\n",
    "* 加减除：```+, -, /, add, sub, div```\n",
    "* 乘法：\n",
    "  * ```*, mul```：点乘\n",
    "  * ```mm, matmul```：矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1000, 1.0000],\n",
      "        [1.0000, 1.1000]])\n",
      "tensor([[1.1000, 1.0000],\n",
      "        [1.0000, 1.1000]])\n",
      "tensor([[ 0.9000, -1.0000],\n",
      "        [-1.0000,  0.9000]])\n",
      "tensor([[ 0.9000, -1.0000],\n",
      "        [-1.0000,  0.9000]])\n",
      "tensor([[10.,  0.],\n",
      "        [ 0., 10.]])\n",
      "tensor([[10.,  0.],\n",
      "        [ 0., 10.]])\n",
      "tensor([[0.1000, 0.0000],\n",
      "        [0.0000, 0.1000]])\n",
      "tensor([[0.1000, 0.0000],\n",
      "        [0.0000, 0.1000]])\n",
      "tensor([[0.1000, 1.0000],\n",
      "        [1.0000, 0.1000]])\n",
      "tensor([[0.1000, 1.0000],\n",
      "        [1.0000, 0.1000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 0], [0, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0.1, 1], [1, 0.1]], dtype=torch.float32)\n",
    "\n",
    "# elementwise addition, substraction, division\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x - y)\n",
    "print(torch.sub(x, y))\n",
    "print(x / y)\n",
    "print(torch.div(x, y))\n",
    "\n",
    "# multiplication\n",
    "# elementwise\n",
    "print(x * y)\n",
    "print(torch.mul(x, y))\n",
    "# matrix product\n",
    "print(torch.mm(x, y))\n",
    "print(torch.matmul(x, y))  # broadcast supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "* 误差还没有反向传播时，grad默认为None，第一次传播之后grad变成Tensor类的实例化对象，而且会一直累加，因此需要手动zero_\n",
    "* 不需要追踪某个tensor的梯度时：\n",
    "  * ```with torch.no_grad():```\n",
    "  * ```x.detach_()```\n",
    "  * ```x.requires_grad_(False)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100: 100%|██████████| 100/100 [00:00<00:00, 1430.42it/s, loss is: 0.412968]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063]], requires_grad=True)\n",
      "tensor([[0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063],\n",
      "        [0.9406, 0.8760, 0.8063]])\n",
      "<SigmoidBackward object at 0x7f6d39dfaa00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(3, 3, requires_grad=True)\n",
    "input = torch.tensor([[0.1, 0.7], [0.2, 0.9], [0.3, 0.6]], dtype=torch.float32)\n",
    "label = torch.tensor([0, 1])\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    output = torch.sigmoid(torch.matmul(weights, input).sum(dim=0))\n",
    "    loss = nn.L1Loss(reduction='mean')(output, label)\n",
    "    \n",
    "    if weights.grad is not None:\n",
    "        weights.grad.zero_()\n",
    "    loss.backward()  # backward() accumulates the gradient for this tensor into .grad attribute\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "    \n",
    "    epoch_loop.set_description(\"Epoch: %d\" % (epoch + 1))\n",
    "    epoch_loop.set_postfix_str(\"loss is: %.6f\" % loss.item())\n",
    "\n",
    "\n",
    "print(weights)\n",
    "weights.requires_grad_(False)  # weights.detach_()\n",
    "print(weights)\n",
    "print(output.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Pipeline\n",
    "* Design model\n",
    "* Construct loss and optimizer\n",
    "* Training loop:\n",
    "  * forward pass: compute prediction and loss\n",
    "  * backward pass: compute gradients\n",
    "  * update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------numpy implementation-----------------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 2054.72it/s, loss = 0.06237932, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 1-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:01<00:00, 16.65it/s, loss = 0.06237914, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 2-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 1640.13it/s, loss = 0.06237914, w = 1.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n",
      "-----------------------------pytorch implementation version 3-----------------------------\n",
      "before training, f(5) = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 20/20 [00:00<00:00, 616.84it/s, loss = 0.06237914, w = 1.92]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after training, f(5) = 9.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# numpy implementation\n",
    "class NumpyImplementation(object):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        self.w = w\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w * x\n",
    "    \n",
    "    def loss(self, y_predicted, y):\n",
    "        return ((y_predicted - y) ** 2).mean()\n",
    "\n",
    "    def grad(self, x, y_predicted, y):\n",
    "        return np.dot(2 * x, (y_predicted - y)) / len(x)\n",
    "\n",
    "    def update_weights(self, grad):\n",
    "        self.w -= self.lr * grad\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        print(\"numpy implementation\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            g = self.grad(x, y_pred, y)\n",
    "            self.update_weights(g)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using autograd to obtain grad\n",
    "class TorchImplementation1(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation1, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 1\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            l.backward()\n",
    "            with torch.no_grad():\n",
    "                self.update_weights(self.w.grad)\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "# pytorch implementation: using optimizer to update weights\n",
    "class TorchImplementation2(NumpyImplementation):\n",
    "    def __init__(self, w, lr, epochs=10):\n",
    "        super(TorchImplementation2, self).__init__(w, lr, epochs)\n",
    "        self.w = torch.tensor(self.w, requires_grad=True)\n",
    "        self.optimizer = torch.optim.SGD([self.w], self.lr)  # 需要更新的权重必须是可迭代的\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x)\n",
    "        y = torch.from_numpy(y)\n",
    "\n",
    "        print(\"pytorch implementation version 2\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self(5):.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l:.8f}, w = {self.w:.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self(5):.3f}')\n",
    "\n",
    "\n",
    "# pytorch implementation: using nn.Linear to build network and nn.MSELoss to compute loss\n",
    "class TorchImplementation3(object):\n",
    "    def __init__(self, lr, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.model = nn.Linear(1, 1, bias=False)\n",
    "        nn.init.constant_(self.model.weight, 0)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        x = torch.from_numpy(x).unsqueeze(-1)\n",
    "        y = torch.from_numpy(y).unsqueeze(-1)\n",
    "\n",
    "        print(\"pytorch implementation version 3\".center(90, '-'))\n",
    "        print(f\"before training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}\")\n",
    "\n",
    "        epoch_loop = tqdm(range(0, self.epochs))\n",
    "        for epoch in epoch_loop:\n",
    "            y_pred = self.model(x)\n",
    "            l = self.loss(y_pred, y)\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "            epoch_loop.set_postfix_str(f'loss = {l.item():.8f}, w = {self.model.state_dict()[\"weight\"].item():.2f}')\n",
    "\n",
    "        print(f'after training, f(5) = {self.model(torch.tensor([5.0])).item():.3f}')\n",
    "\n",
    "\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epoch = 20\n",
    "\n",
    "ni = NumpyImplementation(0.0, learning_rate, num_epoch)\n",
    "ni.train(x, y)\n",
    "\n",
    "ti1 = TorchImplementation1(0.0, learning_rate, num_epoch)\n",
    "ti1.train(x, y)\n",
    "\n",
    "ti2 = TorchImplementation2(0.0, learning_rate, num_epoch)\n",
    "ti2.train(x, y)\n",
    "\n",
    "ti3 = TorchImplementation3(learning_rate, num_epoch)\n",
    "ti3.train(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2000: 100%|██████████| 2000/2000 [00:00<00:00, 8202.00it/s, loss: 75.2696]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=10, random_state=4)\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Linear(x.shape[1], y.shape[1])\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 2000))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        epoch_loop.set_description(f\"epoch: {(epoch + 1):d}\")\n",
    "        epoch_loop.set_postfix_str(f\"loss: {loss.item():.4f}\")\n",
    "\n",
    "y_pred = model(x).detach().cpu().numpy()\n",
    "plt.plot(x_numpy, y_numpy, \"ro\")\n",
    "plt.plot(x_numpy, y_pred, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100: 100%|██████████| 100/100 [00:00<00:00, 4107.55it/s, loss = 0.2591, acc = 0.9386]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "x, y = bc.data, bc.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 100))\n",
    "for epoch in epoch_loop:\n",
    "    y_pred = model(x_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_test)\n",
    "            y_pred_cls = y_pred.round()\n",
    "            acc = y_pred_cls.eq(y_test).sum() / (y_test.shape[0])\n",
    "        epoch_loop.set_description(f'epoch: {epoch + 1}')\n",
    "        epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset & Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10.000: 100%|██████████| 10/10 [00:00<00:00, 14.06it/s, loss = 0.0208, acc = 0.9737]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class LogisticDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        bc = datasets.load_breast_cancer()\n",
    "        x, y = bc.data, bc.target\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        x_train = sc.fit_transform(x_train)\n",
    "        x_test = sc.transform(x_test)\n",
    "\n",
    "        self.x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "        self.x_test = torch.from_numpy(x_test.astype(np.float32))\n",
    "        self.y_train = torch.from_numpy(y_train.astype(np.float32)).unsqueeze(-1)\n",
    "        self.y_test = torch.from_numpy(y_test.astype(np.float32)).unsqueeze(-1)\n",
    "        self.train = train\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.x_train)\n",
    "        else:\n",
    "            return len(self.x_test)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            return self.x_train[index], self.y_train[index]\n",
    "        else:\n",
    "            return self.x_test[index], self.y_test[index]\n",
    "\n",
    "train_dataset = LogisticDataset(train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_dataset = LogisticDataset(train=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(30, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "criterion= nn.BCELoss()\n",
    "\n",
    "epoch_loop = tqdm(range(0, 10), leave=True)\n",
    "for epoch in epoch_loop:\n",
    "    num_batches = len(train_dataloader)\n",
    "    epoch_per_batch = 1.0 / num_batches\n",
    "\n",
    "    for i, (data, target) in enumerate(train_dataloader):\n",
    "        epoch += epoch_per_batch\n",
    "        epoch_loop.set_description(f'epoch: {epoch:.3f}')\n",
    "\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}')\n",
    "    if round(epoch) % 10 == 0:\n",
    "        num_corrects = 0\n",
    "        num_samples = 0\n",
    "        for i, (data, target) in enumerate(test_dataloader):\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(data)\n",
    "                y_pred_cls = y_pred.round()\n",
    "                num_corrects += y_pred_cls.eq(target).sum()\n",
    "                num_samples += y_pred_cls.shape[0]\n",
    "            acc = num_corrects / num_samples\n",
    "            epoch_loop.set_postfix_str(f'loss = {loss.item():.4f}, acc = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Softmax & Crossentropy\n",
    "* PyTorch默认的```nn.CrossEntropyLoss```，等同于```nn.LogSoftmax + nn.NLLLoss```，因此不需要在网络的最后使用```torch.softmax```，并且label的shape是```（n_samples, )```，不是one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41703007\n",
      "torch.Size([2])\n",
      "tensor(0.4170)\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def cross_entropy_loss(x, y):\n",
    "    return -np.dot(y, np.log(x))\n",
    "\n",
    "out = np.array([2.0, 1.0, 0.1], dtype=np.float32)\n",
    "label = np.array([1, 0, 0], dtype=np.float32)\n",
    "pred = softmax(out)\n",
    "print(cross_entropy_loss(pred, label))\n",
    "\n",
    "out = torch.tensor([[2.0, 1.0, 0.1], [1.0, 2.0, 0.1]])  # shape: n_sample * n_cls\n",
    "label = torch.tensor([0, 1])  # shape: n_sample\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(criterion(out, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tensorboard\n",
    "* global_step：可以用于显示同一个tag下不同step的图片，随着epoch的增加，显示不同的预测结果\n",
    "* add_image：tensor或者array，shape：c * h * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from einops import rearrange\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"./runs\")\n",
    "\n",
    "# matplotlib.pyplot.figure\n",
    "fig = plt.figure(1)\n",
    "plt.plot([0, 1, 2], [2, 3, 4], '-')\n",
    "writer.add_figure(\"figure\", fig)\n",
    "\n",
    "# image: c * h * w\n",
    "img = rearrange(cv.imread(\"./materials/panda.jpg\"), 'h w c -> c h w')\n",
    "img2 = rearrange(cv.imread(\"./materials/panda2.jpg\"), 'h w c -> c h w')\n",
    "writer.add_image(\"images\", img, global_step=0)\n",
    "writer.add_image(\"images\", img2, global_step=1)\n",
    "\n",
    "# model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 10),\n",
    "    nn.BatchNorm1d(10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "input = torch.rand(4, 10)\n",
    "writer.add_graph(model, input_to_model=input)\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    writer.add_scalar(\"loss\", 2 * epoch + 1, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Traning\n",
    "* 采用```DistributedDataParallel```训练时，如果从头训练负载相对均衡，而从checkpoint开始训练就出现负载不均衡，可以采用```state = torch.load('xxx.pth', map=torch.device('cpu'))```解决\n",
    "* 保存模型时，注意加```if dist.get_rank() == 0```来保证只在一个线程里面保存，不然后面load的时候会出错\n",
    "* 只需要在一个线程里面执行的操作，比如打印误差，日志等，也可以用```if dist.get_rank() == 0```来实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "len(torch.__all__)\n",
    "len(dir(torch))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a94075a2e62db5dc98a7ce177b0aa497782b90b7701cb2e0b55d059aa447695"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
